{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.lm_rnn import *\n",
    "import sentencepiece as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "btw-nouniq30k  ge2017  shared  shared-wiki  wiki30k\n",
      "dev_v1.4.tsv  test_TIMESTAMP1.tsv  test_TIMESTAMP2.tsv\ttrain_v1.4.tsv\n",
      "dev_v1.4.xml  test_TIMESTAMP1.xml  test_TIMESTAMP2.xml\ttrain_v1.4.xml\n"
     ]
    }
   ],
   "source": [
    "BTW17 = Path(\"../data/btw17\")\n",
    "germeval2017  = Path(\"../data/germeval2017\")\n",
    "WORK = Path(\"../work/\")\n",
    "!ls {WORK}\n",
    "!ls {germeval2017}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv  train.txt  val.csv\r\n"
     ]
    }
   ],
   "source": [
    "wiki=Path(\"../data/wiki/de\")\n",
    "!ls {wiki}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Wiki LM text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_wiki = pd.read_csv(wiki/\"train.csv\", header=None)\n",
    "#np.savetxt(wiki/'train.txt', train_wiki.values, fmt=\"%s\")\n",
    "#!cat {BTW17}/text.txt >> {wiki}/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare germeval 2017 to use wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_germeval(csvfn):\n",
    "    cols=[\"id\", \"text\", \"relevance\", \"sentiment\", \"aspect:polarity\"]\n",
    "    df = pd.read_csv(germeval2017/csvfn , delimiter=\"\\t\", header=None, names=cols, index_col=False)\n",
    "    df.sentiment = pd.Categorical(df.sentiment, categories=['neutral', 'negative', 'positive'])\n",
    "    df.relevance = pd.Categorical(df.relevance, categories=[False, True])\n",
    "    sen = pd.DataFrame(df.text)\n",
    "    sen['sentiment'] = df.sentiment.cat.codes\n",
    "    rel = pd.DataFrame(df.text)\n",
    "    rel['relevance'] = df.relevance.cat.codes\n",
    "    return sen,rel\n",
    "\n",
    "# sen_trn,_ = read_germeval('train_v1.4.tsv')\n",
    "# sen_val,_ = read_germeval('dev_v1.4.tsv')\n",
    "# sen_test1,_ = read_germeval('test_TIMESTAMP1.tsv')\n",
    "# sen_test2,_ = read_germeval('test_TIMESTAMP2.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen,_ = read_germeval('dev_v1.4.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "spp = sp.SentencePieceProcessor()\n",
    "p = WORK/'wiki30k'\n",
    "spp.Load(str( p/ 'tmp' / 'sp.model'))\n",
    "vs = spp.GetPieceSize() #len(itos)\n",
    "spp.SetEncodeExtraOptions(\"bos:eos\")\n",
    "def toIds_raw(l):\n",
    "    return spp.EncodeAsIds(str(l))   # to cover for NaN like on line 164\n",
    "\n",
    "def tokenize(lines):\n",
    "    WORKERS=num_cpus()\n",
    "    results=[]\n",
    "    chunksize=max(1,int(len(lines)/100/WORKERS))\n",
    "    with concurrent.futures.ProcessPoolExecutor() as e:\n",
    "        for result in e.map(toIds_raw, lines, chunksize=chunksize):\n",
    "            results.append(np.array(result))\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,    21,   184,   279,  2155,    21,  1968,  1791,   534,   450,    18, 15560,    16,   140,\n",
       "          14,  2801,  6913,     9, 10759,  3519,    18,   822,     6,     5,  5136,     4,  3807,     4,\n",
       "       19985,     4,   119, 18201,    83,    34,    47,    52,    17,     4,    50,    49,   226,  4504,\n",
       "          87,   110,   165,    79,   203,   183,   187,     3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen['tok'] = tokenize(sen['text'])\n",
    "sen['tok'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn, val, test\n",
    "germeval2017_tmp = WORK/'wikige2017'/'tmp'\n",
    "germeval2017_tmp.mkdir(exist_ok=True, parents=True)\n",
    "!cp {p/ 'tmp' / 'sp.model'} {germeval2017_tmp / 'sp.model'} \n",
    "!cp {p/ 'tmp' / 'sp.vocab'} {germeval2017_tmp / 'sp.vocab'} \n",
    "\n",
    "def convert_germeval(fn, set_name):\n",
    "    sen,_ = read_germeval(fn)\n",
    "    sen['tok'] = tokenize(sen['text'])\n",
    "    np.save(germeval2017_tmp/f'{set_name}_ids.npy' ,np.array(sen['tok']))\n",
    "    np.save(germeval2017_tmp/f'lbl_{set_name}.npy' ,np.array(sen['sentiment']))\n",
    "    return sen['tok']\n",
    "# 'tmp/val_{IDS}_bwd.npy'\n",
    "# 'tmp/trn_{IDS}.npy'\n",
    "# 'tmp/val_{IDS}.npy'\n",
    "# 'tmp/lbl_trn{train_file_id}.npy'\n",
    "# 'tmp/lbl_val.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen_trn,_ = read_germeval('train_v1.4.tsv')\n",
    "# sen_val,_ = read_germeval('dev_v1.4.tsv')\n",
    "# sen_test1,_ = read_germeval('test_TIMESTAMP1.tsv')\n",
    "# sen_test2,_ = read_germeval('test_TIMESTAMP2.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = [[]]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text[0] = convert_germeval('train_v1.4.tsv', 'trn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text[1] =  [] # do not add val set to all text\n",
    "val = convert_germeval('dev_v1.4.tsv', 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"id\", \"text\", \"relevance\", \"sentiment\", \"aspect:polarity\"]\n",
    "df = pd.read_csv(germeval2017/'test_TIMESTAMP1.tsv' , delimiter=\"\\t\", header=None, names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_text[2] = convert_germeval('test_TIMESTAMP1.tsv', 'test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_text[3] = convert_germeval('test_TIMESTAMP2.tsv', 'test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20941,), (0,), (2566,), (1842,)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.array(a).shape for a in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_np = np.concatenate(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(germeval2017_tmp/f'trn_ids_all.npy' ,all_text_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://plus.google.com/106967535142671617878/posts/5x5NBQhH2Pv\tnull\tfalse\tneutral\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 165 {germeval2017}/dev_v1.4.tsv | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence piece statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal length of sentence in tokens 6663\n",
      "AVG Length in tokens 136.91369969040247\n",
      "Length of valuation set 2584\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximal length of sentence in tokens\", max([len(s) for s in val]))\n",
    "print(\"AVG Length in tokens\", sum([len(s) for s in val])/len(val))\n",
    "print(\"Length of valuation set\", len(val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
